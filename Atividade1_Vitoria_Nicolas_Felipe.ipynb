{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pergunta: What does Keras Tokenizer method actually do?"
      ],
      "metadata": {
        "id": "eQfFWdhBiMUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6IQ9FQU9iLxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função fit_on_texts() cria o vocabulário e o índice chamado word_index com base na frequência que essa palavra ocorre."
      ],
      "metadata": {
        "id": "73py9OjKcE4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quanto menor for o número de word_index, mais frequente é a palavra (o índice 0 é reservado para padding, que é o preenchimento em redes neurais)."
      ],
      "metadata": {
        "id": "4YSIGZiucda4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os métodos são separados pois o \"fit\" (cria o vocabulário) apenas uma vez (com os dados de treino), mas usa texts_to_sequences() muitas vezes (nos dados de treino, validação e teste). Assim, garante-se consistência no pré-processamento."
      ],
      "metadata": {
        "id": "7LtwZgmZn7lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo da pergunta do StackOverflow"
      ],
      "metadata": {
        "id": "ez8uEkXUj53l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "print(\"Vocabulário (word_index):\")\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "print(\"\\nSequências:\")\n",
        "print(tokenizer.texts_to_sequences(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf1pxaEuV-wc",
        "outputId": "c609ce8e-ef4a-4441-f194-3121cd9f3d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário (word_index):\n",
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
            "\n",
            "Sequências:\n",
            "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Tokenizer do Keras cria um mapeamento palavra -> indice com base na frequência"
      ],
      "metadata": {
        "id": "bu3yBh6HYmuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É importante treinar o Tokenizer apenas uma vez e reutilizar o vocabulário, afim de evitar inconsistências no modelo."
      ],
      "metadata": {
        "id": "0xi0_H_yaKJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outro exemplo do Keras Tokenizer"
      ],
      "metadata": {
        "id": "ae9QRljPkA7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Never jump over the lazy dog quickly.\",\n",
        "    \"The dog is not just lazy, it is clever and loyal.\"\n",
        "]\n",
        "\n",
        "# Criar e treinar o Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "print(\"Vocabulário (word_index):\")\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "print(\"\\nSequências:\")\n",
        "print(tokenizer.texts_to_sequences(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-6_cMwhkJTZ",
        "outputId": "af01e008-43b6-48f1-db49-faf17b3d5908"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário (word_index):\n",
            "{'the': 1, 'lazy': 2, 'dog': 3, 'over': 4, 'is': 5, 'quick': 6, 'brown': 7, 'fox': 8, 'jumps': 9, 'never': 10, 'jump': 11, 'quickly': 12, 'not': 13, 'just': 14, 'it': 15, 'clever': 16, 'and': 17, 'loyal': 18}\n",
            "\n",
            "Sequências:\n",
            "[[1, 6, 7, 8, 9, 4, 1, 2, 3], [10, 11, 4, 1, 2, 3, 12], [1, 3, 5, 13, 14, 2, 15, 5, 16, 17, 18]]\n"
          ]
        }
      ]
    }
  ]
}